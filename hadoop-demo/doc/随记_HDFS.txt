HDFS概述
    HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件。其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。
    HDFS的使用场景：适合一次写入，多次读出的场景。一个文件经过创建、写入和关闭之后就不需要改变。
    HDFS优点
        高容错性
            数据自动保存多个副本。它通过增加副本的形式，提供容错性。
            某一个副本丢失以后，它可以自动恢复。
        适合处理大数据
            数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据。
            文件规模：能够处理百万规模以上的文件数量。
        可构建在廉价机器上，通过多副本机制，提高可靠性。
    HDFS缺点
        不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。
        无法高效的对大量小文件进行存储
            存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的。
            小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。
        不支持并发写入、文件随机修改。
            一个文件只能有一个写，不允许多个线程同时写。
            仅支持数据追加（append），不支持文件随机修改。
HDFS组成架构
    NameNode（NN）：就是Master，它是一个管理者。
        管理HDFS的名称空间
        配置副本策略
        管理数据块（Block）映射信息
    DataNode：就是Salve。NameNode下达命令，DataNode执行实际的操作。
        存储实际的数据块
        执行数据块的读写操作
    Client：就是客户端。
        文件切分。文件上传HDFS的时候，Client将文件一个一个的Block，然后进行上传。
        与NameNode交互，获取文件的位置信息。
        与DataNode交互，读取或者写入数据。
        Client提供一些命令来管理HDFS，比如NameNode格式化。
        Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作。
    Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。
        辅助NameNode，分担其工作量，比如定期合并FsImage和Edits，并推送给NameNode。
        在紧急情况下，可辅助恢复NameNode。
HDFS文件块大小
    HDFS的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定，默认大小在Hadoop2.x/Hadoop3.x版本中是128M，1.x版本中是64M。
        如果寻址时间约为10ms，即查找到目标Block的时间为10ms。
        寻址时间为传输时间的1%时，则为最佳状态（专家）。因此，传输时间 = 10ms / 0.01 = 1000ms = 1s。
        而目前磁盘的传输速率普遍为100MB/s，所以Block大小 = 1s * 100MB/s = 100MB，即应取128MB。
    HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。HDFS块的大小设置主要取决于磁盘传输速率。
HDFS的Shell操作
    前置条件
        hadoop fs -mkdir /sxydh
    上传文件
        moveFromLocal：从本地剪切粘贴到HDFS。
            hadoop fs -moveFromLocal /opt/module/tmp/moveFromLocal.txt /sxydh
        copyFromLocal：从本地拷贝文件到HDFS。
            hadoop fs -copyFromLocal /opt/module/tmp/copyFromLocal.txt /sxydh
        put：等同于copyFromLocal。
            略
        appendToFile：追加一个文件到已经存在的文件末尾。
            hadoop fs -appendToFile /opt/module/tmp/appendToFile.txt /sxydh/copyFromLocal.txt
    下载文件
        copyToLocal：从HDFS拷贝到本地。
            hadoop fs -copyToLocal /sxydh/copyFromLocal.txt /opt/module/tmp/copyToLocal.txt
        get：等同于copyToLocal。
            略
    其它命令
        ls：显示目录信息。
        cat：显示文件内容
        chgrp/chmod/chown：与Linux文件系统中的用法一样，修改文件所属权限。
        mkdir：创建路径。
        cp：从HDFS的一个路径拷贝到HDFS的另一个路径。
        mv：在HDFS目录中移动文件。
        tail：显示一个文件的末尾1kb的数据。
        rm：删除文件或文件夹。
        rm -r：递归删除目录及目录里面内容。
        du：统计文件夹的大小信息。
        setrep：设置HDFS中文件的副本数量。
            hadoop fs -setrep 10 /sxydh/copyFromLocal.txt
                这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。如果当前集群只有3台设备，最多也就3个副本，只有节点数增加到10台时，副本数才能达到10。
HDFS的API操作（Windows）
    前置条件 
        安装JDK8，配置JDK环境变量。
        HADOOP_HOME环境变量
            指向参考：C:\Users\Administrator\AppData\Local\apache-hadoop-3.1.0。
            apache-hadoop-3.1.0是Windows依赖，从这里下载：https://github.com/s911415/apache-hadoop-3.1.0-winutils/archive/refs/heads/master.zip。
            配置好HADOOP_HOME之后，还需要手动执行程序：apache-hadoop-3.1.0\bin\winutils.exe，不报错即表示成功，如果报错可以尝试安装以下安装包：https://download.visualstudio.microsoft.com/download/pr/10912041/cee5d6bca2ddbcd039da727bf4acb48a/vcredist_x64.exe。
            PATH环境变量附加：%HADOOP_HOME%\bin。
    操作API
        详见hadoop-demo/hdfsclient-demo/src/main/java/cn/net/bhe/hdfsclientdemo
HDFS写数据流程
    创建HDFS的分布式文件系统客户端DistributedFileSystem，并向NameNode请求上传文件。
    NameNode检查目录树是否可以创建文件，并响应客户端是否可以上传文件。
        检查权限
        检查目录结构（是否存在）
    客户端向NameNode请求上传第一个Block（0~128M），NameNode返回可以存储数据的DataNode节点（存储节点选择：本地节点，其它机架节点）。
    客户端创建数据流FSDataOutputStream，以串行方式请求建立Block传输通道DataNode1，DataNode2，...。
    客户端传输数据包Packet，DataNode应答ACK。数据包以64k为一个单元，每个单元的数据队列由多个数据块组成（512字节的chunk + 4字节的chunksum）。
HDFS节点距离计算
    两节点之间的距离 = 两节点到达共同祖先的距离之和
        同一节点上的进程：Distance(d1/r1/n0, d1/r1/n0) = 0。
        同一机架上的不同节点：Distance(d1/r1/n0, d1/r1/n1) = 2。
        同一数据中心不同机架上的节点：Distance(d1/r1/n0, d1/r2/n0) = 4。
        不同数据中心的节点：Distance(d1/r1/n0, d2/r1/n0) = 6。
HDFS存储节点选择
    第一个副本在客户端所处的节点上。如果客户端在集群外，随机选一个。
    第二个副本在另一个机架的随机一个节点
    第三个副本在第二个副本所在机架的随机节点
    源码参考
        BlockPlacementPolicyDefault#chooseTargeInOrder
HDFS读数据流程
    创建HDFS的分布式文件系统客户端DistributedFileSystem，并向NameNode请求下载文件。
    NameNode返回目标文件的元数据，包含数据块信息、数据节点信息等。
    客户端创建数据流FSDatainputStream，从目标数据节点读取数据。
HDFS NN和2NN工作机制
    思考
        NameNode中的元数据是存储在哪里的？
    概述
        首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。
        这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。
        但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNameNode，专门用于FsImage和Edits的合并。
    工作机制
        假设NameNode节点内存128G（每个Block占元数据150byte）
        NodeNode启动时会加载编辑日志edits_inprogress_001和镜像文件fsimage到内存
        客户端增删改元数据时，NameNode记录操作日志到edits_inprogress_001（磁盘内）。
        SecondaryNameNode请求NameNode是否可以执行CheckPoint，CheckPoint触发条件：
            定时时间到
            Edits中的数据满了
        NameNode滚动正在写的Edits文件：将edits_inprogress_001复制一份为edits_inprogress_002，edits_inprogress_001重命名为edits_001，后续操作日志记录在edits_inprogress_002。
        SecondaryNameNode节点将edits_001和fsimage拉取到本地，并加载到内存，然后将edits_001记录的操作日志合并到fsimage，生成新的fsimage.chkpoint，最后将fsimage.chkpoint拷贝到NameNode。
        NameNode节点将fsimag.chkpoint重命名为fsimage，覆盖原来的fsimage，与edits_inprogress_002共同形成最新完整的元数据。
    其它
        Edits文件和FsImage文件存放在<HADOOP_HOME>/data/dfs/name/current。
HDFS FsImage镜像文件
    NameNode被格式化后，将在<HADOOP_HOME>/data/dfs/name/current目录中产生如下文件：
        fsimage_0000000000000000000
        fsimage_0000000000000000000.md5
        seen_txid
        VERSION
    FsImage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。
    Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。
    seen_txid文件：保存的是一个数字，就是最后一个edits_的数字。
    每次NameNode启动的时候都会将FsImage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据是最新的、同步的，可以看成NameNode启动的时候就将FsImage和Edits文件进行了合并。
HDFS检查点时间设置
    每隔一小时执行一次CheckPoint
        hdfs-default.xml 
            <property>
                <name>dfs.namenode.checkpoint.period</name>
                <value>3600s</value>
            </property>
    一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次CheckPoint。
        hdfs-default.xml 
            <property>
                <name>dfs.namenode.checkpoint.txns</name>
                <value>1000000</value>
                <description>操作动作次数</description>
            </property>
            <property>
                <name>dfs.namenode.checkpoint.check.period</name>
                <value>60s</value>
                <description>1分钟检查一次操作次数</description>
            </property>
HDFS DN工作机制
    DataNode主要存储数据块Block，Block中含有数据、数据长度、校验和、时间戳等信息。
    DataNode启动后向NameNode注册
    DataNode每6小时向NameNode上报所有块信息
        hdfs-default.xml
            <property>
                <name>dfs.blockreport.intervalMsec</name>
                <value>21600000</value>
                <description>DN向NN汇报当前节点信息的时间间隔，默认6小时</description>
            </property>
            <property>
                <name>dfs.datanode.directoryscan.interval</name>
                <value>21600s</value>
                <description>DN扫描自己节点块信息列表的时间间隔，默认6小时</description>
            </property>
    DataNode每3秒向NameNode发起一次心跳通信，心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一台机器，或删除某个数据块。
        超过10分钟 + 30秒没有收到DataNode的心跳，则认为该节点不可用。
        Timeout = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval 
            dfs.namenode.heartbeat.recheck-interval：默认5分钟。
                hdfs-site.xml
                    <property>
                        <name>dfs.namenode.heartbeat.recheck-interval</name>
                        <value>300000</value>
                    </property>
            dfs.heartbeat.interval：默认3秒。
                hdfs-site.xml
                    <property>
                        <name>dfs.heartbeat.interval</name>
                        <value>3</value>
                    </property>
HDFS数据完整性
    当DataNode读取Block的时候，它会计算CheckSum，如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。
    Client读取其它DataNode上的Block
    常见的校验算法CRC（32）、MD5（128）、SHA1（160）。
    DataNode在其文件创建后周期验证CheckSum。
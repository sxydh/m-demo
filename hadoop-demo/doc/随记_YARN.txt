概述
    YARN是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。
    YARN主要由以下组件构成：
        ResourceManager
            处理客户端请求
            监控NodeManager
            启动或监控ApplicationMaster
            资源的分配与调度
        NodeManager
            管理单个节点上的资源
            处理来自ResourceManager的命令
            处理来自ApplicationMaster的命令
        ApplicationMaster
            为应用程序申请资源并分配给内部的任务
            任务的监控与容错
        Container
            是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。
YARN工作机制
    Mr程序提交到客户端所在的节点
        job.waitForCompletion(true);
            ...
                YARNRunner#submitJob
    客户端向ResourceManager申请一个Application，ResourceManager返回Application资源提交路径hdfs://.../.staging以及applicaton_id。
    客户端提交Job相关资源（例如切片信息Job.split，参数信息Job.xml，程序包Demo.jar等）到资源提交路径，申请运行MrAppMaster。
    ResourceManager将客户端请求初始化成一个Task，放入FIFO调度队列。
    NodeManager1从ResourceManager领取Task任务，创建容器Container，容器包含CPU和RAM等。然后启动MrAppMaster进程，MrAppMaster从资源提交路径下载Job相关资源到本地，读取切片等信息。最后向ResourceManager申请运行MapTask。
    NodeManager2.1，2.2，...，从ResourceManager领取MapTask任务，创建容器以运行任务，容器包含CPU、RAM和任务Jar包等。运行MapTask的不同容器可能在不同的NodeManager，也可能在一个NodeManager内。
    NodeManager1的MrAppMaster发送程序启动脚本到各MapTask的NodeManager，NodeManager开启YarnChild进程运行MapTask。
    MapTask执行完毕后，输出溢写文件。
    NodeManager1的MrAppMaster向ResourceManager申请资源，创建容器以运行ReduceTask。ReduceTask从MapTask拉取数据，执行Reducer业务逻辑。
    程序运行完毕后，NodeManager1的MrAppMaster向ResourceManager注销自己，释放相关资源。
YARN调度器算法
    目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler（容量）和Fair Scheduler（公平）。
    Hadoop3.1.3默认的资源调度器是Capacity Sheduler。 
        yarn-default.xml 
            <property>
                <name>yarn.resourcemanager.scheduler.class</property>
                <value>org.apache.hadoop.yarn.server.resourcemanager.sheduler.capacity.CapacityScheduler</value>
            </property>
    FIFO调度器
        单队列，根据提交作业的先后顺序，先来先服务。
    Capacity Scheduler容量调度器
        多队列：每个队列可配置一定的资源量。
            队列间资源分配：采用使用深度优先算法，即优先给资源占用率最低的队列分配资源。
            队列内作业资源分配：采用FIFO调度策略。
            容器资源分配：按照容器的优先级分配资源，如果优先级相同，按照数据本地性原则。
                任务和数据在同一节点
                任务和数据在同一机架
                任务和数据不在同一节点也不在同一机架
        容量保证：管理员可为每个队列设置资源最低保证和资源使用上限。
        灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其它队列借调的资源会归还给该队列。
        多租户
            支持多用户共享集群和多应用程序同时运行
            为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。
    Fair Scheduler公平调度器
        公平调度器同队列所有任务共享资源，在时间尺度上获得公平的资源。某一时刻一个作业应获资源和实际获取资源的差距叫缺额。

        与容量调度器相同点：多队列、容量保证、灵活性和多租户。
        与容量调度器不同点
            核心调度策略不同
                容量调度器：优先选择资源利用率低的队列。
                公平调度器：优先选择对资源的缺额比例大的队列。
            每个队列可以单独设置资源分配方式
                容量调度器：FIFO、DRF。
                公平调度器：FIFO、FAIR、DRF。

        公平调度器队列资源分配方式
            FIFO策略
                公平调度器每个队列资源分配策略如果选择FIFO的话，此时公平调度器相当于上面讲过的容量调度器。
            Fair策略
                Fair策略（默认）是一种基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源；如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。
                具体资源分配流程和容量调度器一致（选择队列、选择作业、选择容器，以上三步，每一步都是按照公平策略分配资源）

        实际最小资源份额：mindshare=Min(资源需求量, 配置的最小资源)。
        是否饥饿：isNeedy=资源使用量<mindshare。
        资源分配占比：minShareRatio=资源使用量/Max(mindshare, 1)。
        资源使用权重比：useToWeightRatio=资源使用量/权重。
        分别计算比较对象的（实际最小资源份额、是否饥饿、资源分配占比、资源使用权重比）
            如果一部分饥饿，另一部分不饥饿，则饥饿的优先。
            如果都饥饿，则资源分配比小者优先。如果资源分配比相同，则按照提交时间正序。
            如果都不饥饿，则资源使用权重比小者优先。如果资源使用权重比相同，则按照提交时间正序。
        
        队列资源分配 
            假设集群总资源100，队列和需求资源分别是：queueA（20），queueA（50），queueA（30）。
            第一次分配 
                100/3=33.33
                queueA：分33.33，多12.33。
                queueB：分33.33，少16.67。
                queueC：分33.33，多3.33。
            第二次分配
                (13.33+3.33)/1=16.66
                queueA：分20。
                queueB：分33.33+16.66=50。
                queueC：分30。
        作业资源分配
            不加权
                假设某队列总资源12，Job和需求资源分别是：job（1），job（2），job（6），job（5）。
                第一次分配 
                    12/4=3
                    job1：分3，多2。
                    job2：分3，多1。
                    job3：分3，少3。
                    job4：分3，少2。
                第二次分配 
                    (1+2)/2=1.5
                    job1：分1。
                    job2：分2。
                    job3：分3+1.5=4.5。
                    job4：分3+1.5=4.5。
                ...
                直到没有多出的资源
            加权重
                假设某队列总资源16，Job、需求资源和权重分别是：job（4，5），job（2，8），job（10，1），job（4，2）。
                第一次分配 
                    16/(5+8+1+2)=1
                    job1：分5，多1。
                    job2：分8，多6。
                    job3：分1，少9。
                    job4：分2，少2。
                第二次分配 
                    (1+6)/(1+2)=7/3
                    job1：分4。
                    job2：分2。
                    job3：分1+7/3*1=3.33，少6.67。
                    job4：分2+7/3*2=6.66，多2.66。
                第三次分配
                    2.66/1=2.66
                    job1：分4。
                    job2：分2。
                    job3：分6，少4。
                    job4：分4。
                ...
                直到没有多出的资源

        DRF策略
            DRF（Dominant Resource Fairness），我们之前说的资源，都是单一标准，例如只考虑内存（也是YARN默认的情况）。但是很多时候我们的资源有很多种，例如内存，CPU，网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。
            那么在YARN中，我们用DRF来决定如何调度：
                假设集群一共有100CPU和10T内存，应用A需要2CPU和300GB内存，应用B需要6CPU和100GB内存，则两个应用需要资源分别是A（2%CPU，3%内存）和B（6%CPU，1%内存）。这就意味着A是内存主导，B是CPU主导。针对这种情况，我们可以选择DRF策略对不同应用进行不同资源（CPU和内存）的一个不同比例的限制。
YARN常用命令
    查看任务列表
        yarn application -list [-appStates <states>]
            appStates：ALL，NEW，NEW_SAVING，SUBMITTED，ACCEPTED，RUNNING，FINISHED，FAILED，KILLED。
    查看尝试运行任务
        yarn applicationattempt -list <application_id>
    查看尝试运行任务状态
        yarn applicationattempt -status <application_attempt_id>
    结束指定任务
        yarn application -kill <application_id>
    查看任务日志
        yarn logs -applicationId <application_id> -containerId <container_id>
    查看容器列表
        yarn container -list <application_attempt_id>
    查看容器状态
        yarn container -status <container_id>
    查看节点列表
        yarn node -list -all
    更新配置信息
        yarn rmadmin -refreshQueues
    查看队列状态
        yarn queue -status -<queue_name>
YARN生产环境核心参数配置
    ResourceManager相关
        yarn.resourcemanager.scheduler.class：配置调度器，默认容量调度器。
        yarn.resourcemanager.sheduler.client.thread-count：ResourceManager处理调度器请求的线程数量，默认50。
    NodeManager相关 
        yarn.nodemanager.resource.detect-hardware-capabilities：是否让YARN自己检测硬件进行配置，默认false。
        yarn.nodemanager.resource.count-logical-processors-as-cores：是否将虚拟核数当作CPU核数，默认false。
        yarn.nodemanager.resource.pcores-vcores-multiplier：虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2，默认1.0。
        yarn.nodemanager.resource.memory-mb：NodeManager使用内存，默认8G。
        yarn.nodemanager.resource.system-reserved-memory-mb：NodeManager为系统保留多少内存，与上一个参数配置二选一。
        yarn.nodemanager.resource.cpu-vcores：NodeManager使用CPU核数，默认8个。
        yarn.nodemanager.pmem-check-enabled：是否开启物理内存检查限制Container，默认打开。
        yarn.nodemanager.vmem-check-enabled：是否开启虚拟内存检查限制Container，默认打开。
        yarn.nodemanager.vmem-pmem-ratio：虚拟内存物理内存比例，默认2.1。
    Container相关 
        yarn.scheduler.minimum-allocation-mb：容器最小内存，默认1G。
        yarn.scheduler.maximum-allocation-mb：容器最大内存，默认8G。
        yarn.scheduler.minimum-allocation-vcores：容器最小CPU核数，默认1个。
        yarn.scheduler.maximum-allocation-vcores：容器最大CPU核数，默认4个。
    案例实操
        假设从1G数据中，统计每个单词出现次数。
        服务器3台，每台配置4G内存，4核CPU，4线程。
        1G/128M=8个MapTask，1个ReduceTask，1个MrAppMaster。
        平均每个节点运行10个/3台≈3个任务
        修改yarn-site.xml配置参数如下：
            <!-- 选择调度器，默认容量。 -->
            <property>
                <name>yarn.resourcemanager.scheduler.class</name>
                <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
            </property>
            <!-- ResourceManager处理调度器请求的线程数量，默认50。如果提交的任务数大于50，可以增加该值，但是不能超过3台*4线程=12线程（去除其它应用程序实际不能超过8）。 -->
            <property>
                <name>yarn.resourcemanager.sheduler.client.thread-count</name>
                <value>8</value>
            </property>
            <!-- 是否让YARN自动检测硬件进行配置，默认是false。如果该节点有很多其它应用程序，建议手动配置，否则采用自动。 -->
            <property>
                <name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
                <value>false</value>
            </property>
            <!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数。 -->
            <property>
                <name>yarn.nodemanager.resource.count-logical-processors-as-cores</name>
                <value>false</value>
            </property>
            <!-- 虚拟核数和物理核数乘数，默认是1.0。 -->
            <property>
                <name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
                <value>1.0</value>
            </property>
            <!-- NodeManager使用内存数，默认8G，修改为4G。 -->
            <property>
                <name>yarn.nodemanager.resource.memory-mb</name>
                <value>4096</value>
            </property>
            <!-- NodeManager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个。 -->
            <property>
                <name>yarn.nodemanager.resource.cpu-vcores</name>
                <value>4</value>
            </property>
            <!-- 容器最小内存，默认1G。 -->
            <property>
                <name>yarn.scheduler.minimum-allocation-mb</name>
                <value>1024</value>
            </property>
            <!-- 容器最大内存，默认8G，修改为2G。 -->
            <property>
                <name>yarn.scheduler.maximum-allocation-mb</name>
                <value>2048</value>
            </property>
            <!-- 容器小CPU核数，默认1个。 -->
            <property>
                <name>yarn.scheduler.minimum-allocation-vcores</name>
                <value>1</value>
            </property>
            <!-- 容器大CPU核数，默认4个，修改为2个。 -->
            <property>
                <name>yarn.scheduler.maximum-allocation-vcores</name>
                <value>2</value>
            </property>
            <!-- 虚拟内存检查，默认代开，修改为关闭。 -->
            <property>
                <name>yarn.nodemanager.vmem-check-enabled</name>
                <value>false</value>
            </property>
            <!-- 虚拟内存和物理内存设置比例，默认2.1。 -->
            <property>
                <name>yarn.nodemanager.vmem-pmem-ratio</name>
                <value>2.1</value>
            </property>
        分发配置到各服务器（若其它服务器硬件配置不一样，则需要单独配置）
            略
        重启集群 
            略
        运行程序并观察任务执行页面
            略
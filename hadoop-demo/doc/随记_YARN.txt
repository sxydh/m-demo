概述
    YARN是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。
    YARN主要由以下组件构成：
        ResourceManager
            处理客户端请求
            监控NodeManager
            启动或监控ApplicationMaster
            资源的分配与调度
        NodeManager
            管理单个节点上的资源
            处理来自ResourceManager的命令
            处理来自ApplicationMaster的命令
        ApplicationMaster
            为应用程序申请资源并分配给内部的任务
            任务的监控与容错
        Container
            是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。
YARN工作机制
    MapReduce程序提交到客户端所在的节点
        job.waitForCompletion(true);
            ...
                YARNRunner#submitJob
    客户端向ResourceManager申请一个Application，ResourceManager返回Application资源提交路径hdfs://.../.staging以及applicaton_id。
    客户端提交Job相关资源（例如切片信息Job.split，参数信息Job.xml，程序包Demo.jar等）到资源提交路径，申请运行MrAppMaster。
    ResourceManager将客户端请求初始化成一个Task，放入FIFO调度队列。
    NodeManager1从ResourceManager领取Task任务，创建容器Container，容器包含CPU和RAM等。然后启动MrAppMaster进程，MrAppMaster从资源提交路径下载Job相关资源到本地，读取切片等信息。最后向ResourceManager申请运行MapTask。
    NodeManager2.1，2.2，...，从ResourceManager领取MapTask任务，创建容器以运行任务，容器包含CPU、RAM和任务Jar包等。运行MapTask的不同容器可能在不同的NodeManager，也可能在一个NodeManager内。
    NodeManager1的MrAppMaster发送程序启动脚本到各MapTask的NodeManager，NodeManager开启YarnChild进程运行MapTask。
    MapTask执行完毕后，输出溢写文件。
    NodeManager1的MrAppMaster向ResourceManager申请资源，创建容器以运行ReduceTask。ReduceTask从MapTask拉取数据，执行Reducer业务逻辑。
    程序运行完毕后，NodeManager1的MrAppMaster向ResourceManager注销自己，释放相关资源。
YARN调度器算法
    目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler（容量）和Fair Scheduler（公平）。
    Hadoop3.1.3默认的资源调度器是Capacity Scheduler。 
        yarn-default.xml 
            <property>
                <name>yarn.resourcemanager.scheduler.class</property>
                <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
            </property>
    FIFO调度器
        单队列，根据提交作业的先后顺序，先来先服务。
    Capacity Scheduler容量调度器
        多队列：每个队列可配置一定的资源量。
            队列间资源分配：采用使用深度优先算法，即优先给资源占用率最低的队列分配资源。
            队列内作业资源分配：采用FIFO调度策略。
            容器资源分配：按照容器的优先级分配资源，如果优先级相同，按照数据本地性原则。
                任务和数据在同一节点
                任务和数据在同一机架
                任务和数据不在同一节点也不在同一机架
        容量保证：管理员可为每个队列设置资源最低保证和资源使用上限。
        灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其它队列借调的资源会归还给该队列。
        多租户
            支持多用户共享集群和多应用程序同时运行
            为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。
    Fair Scheduler公平调度器
        公平调度器同队列所有任务共享资源，在时间尺度上获得公平的资源。某一时刻一个作业应获资源和实际获取资源的差距叫缺额。

        与容量调度器相同点：多队列、容量保证、灵活性和多租户。
        与容量调度器不同点
            核心调度策略不同
                容量调度器：优先选择资源利用率低的队列。
                公平调度器：优先选择对资源的缺额比例大的队列。
            每个队列可以单独设置资源分配方式
                容量调度器：FIFO、DRF。
                公平调度器：FIFO、FAIR、DRF。

        公平调度器队列资源分配方式
            FIFO策略
                公平调度器每个队列资源分配策略如果选择FIFO的话，此时公平调度器相当于上面讲过的容量调度器。
            Fair策略
                Fair策略（默认）是一种基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源；如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。
                具体资源分配流程和容量调度器一致（选择队列、选择作业、选择容器，以上三步，每一步都是按照公平策略分配资源）

        实际最小资源份额：mindshare=Min(资源需求量, 配置的最小资源)。
        是否饥饿：isNeedy=资源使用量<mindshare。
        资源分配占比：minShareRatio=资源使用量/Max(mindshare, 1)。
        资源使用权重比：useToWeightRatio=资源使用量/权重。
        分别计算比较对象的（实际最小资源份额、是否饥饿、资源分配占比、资源使用权重比）
            如果一部分饥饿，另一部分不饥饿，则饥饿的优先。
            如果都饥饿，则资源分配比小者优先。如果资源分配比相同，则按照提交时间正序。
            如果都不饥饿，则资源使用权重比小者优先。如果资源使用权重比相同，则按照提交时间正序。
        
        队列资源分配 
            假设集群总资源100，队列和需求资源分别是：queueA（20），queueA（50），queueA（30）。
            第一次分配 
                100/3=33.33
                queueA：分33.33，多12.33。
                queueB：分33.33，少16.67。
                queueC：分33.33，多3.33。
            第二次分配
                (13.33+3.33)/1=16.66
                queueA：分20。
                queueB：分33.33+16.66=50。
                queueC：分30。
        作业资源分配
            不加权
                假设某队列总资源12，Job和需求资源分别是：job（1），job（2），job（6），job（5）。
                第一次分配 
                    12/4=3
                    job1：分3，多2。
                    job2：分3，多1。
                    job3：分3，少3。
                    job4：分3，少2。
                第二次分配 
                    (1+2)/2=1.5
                    job1：分1。
                    job2：分2。
                    job3：分3+1.5=4.5。
                    job4：分3+1.5=4.5。
                ...
                直到没有多出的资源
            加权重
                假设某队列总资源16，Job、需求资源和权重分别是：job（4，5），job（2，8），job（10，1），job（4，2）。
                第一次分配 
                    16/(5+8+1+2)=1
                    job1：分5，多1。
                    job2：分8，多6。
                    job3：分1，少9。
                    job4：分2，少2。
                第二次分配 
                    (1+6)/(1+2)=7/3
                    job1：分4。
                    job2：分2。
                    job3：分1+7/3*1=3.33，少6.67。
                    job4：分2+7/3*2=6.66，多2.66。
                第三次分配
                    2.66/1=2.66
                    job1：分4。
                    job2：分2。
                    job3：分6，少4。
                    job4：分4。
                ...
                直到没有多出的资源

        DRF策略
            DRF（Dominant Resource Fairness），我们之前说的资源，都是单一标准，例如只考虑内存（也是YARN默认的情况）。但是很多时候我们的资源有很多种，例如内存，CPU，网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。
            那么在YARN中，我们用DRF来决定如何调度：
                假设集群一共有100CPU和10T内存，应用A需要2CPU和300GB内存，应用B需要6CPU和100GB内存，则两个应用需要资源分别是A（2%CPU，3%内存）和B（6%CPU，1%内存）。这就意味着A是内存主导，B是CPU主导。针对这种情况，我们可以选择DRF策略对不同应用进行不同资源（CPU和内存）的一个不同比例的限制。
YARN常用命令
    查看任务列表
        yarn application -list [-appStates <states>]
            appStates：ALL，NEW，NEW_SAVING，SUBMITTED，ACCEPTED，RUNNING，FINISHED，FAILED，KILLED。
    查看尝试运行任务
        yarn applicationattempt -list <application_id>
    查看尝试运行任务状态
        yarn applicationattempt -status <application_attempt_id>
    结束指定任务
        yarn application -kill <application_id>
    查看任务日志
        yarn logs -applicationId <application_id> -containerId <container_id>
    查看容器列表
        yarn container -list <application_attempt_id>
    查看容器状态
        yarn container -status <container_id>
    查看节点列表
        yarn node -list -all
    更新配置信息
        yarn rmadmin -refreshQueues
    查看队列状态
        yarn queue -status -<queue_name>
YARN生产环境核心参数配置
    ResourceManager相关
        yarn.resourcemanager.scheduler.class：配置调度器，默认容量调度器。
        yarn.resourcemanager.scheduler.client.thread-count：ResourceManager处理调度器请求的线程数量，默认50。
    NodeManager相关 
        yarn.nodemanager.resource.detect-hardware-capabilities：是否让YARN自己检测硬件进行配置，默认false。
        yarn.nodemanager.resource.count-logical-processors-as-cores：是否将虚拟核数当作CPU核数，默认false。
        yarn.nodemanager.resource.pcores-vcores-multiplier：虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2，默认1.0。
        yarn.nodemanager.resource.memory-mb：NodeManager使用内存，默认8G。
        yarn.nodemanager.resource.system-reserved-memory-mb：NodeManager为系统保留多少内存，与上一个参数配置二选一。
        yarn.nodemanager.resource.cpu-vcores：NodeManager使用CPU核数，默认8个。
        yarn.nodemanager.pmem-check-enabled：是否开启物理内存检查限制Container，默认打开。
        yarn.nodemanager.vmem-check-enabled：是否开启虚拟内存检查限制Container，默认打开。
        yarn.nodemanager.vmem-pmem-ratio：虚拟内存物理内存比例，默认2.1。
    Container相关 
        yarn.scheduler.minimum-allocation-mb：容器最小内存，默认1G。
        yarn.scheduler.maximum-allocation-mb：容器最大内存，默认8G。
        yarn.scheduler.minimum-allocation-vcores：容器最小CPU核数，默认1个。
        yarn.scheduler.maximum-allocation-vcores：容器最大CPU核数，默认4个。
YARN案例实操    
    核心参数配置
        假设从1G数据中，统计每个单词出现次数。
        服务器3台，每台配置4G内存，4核CPU，4线程。
        1G/128M=8个MapTask，1个ReduceTask，1个MrAppMaster。
        平均每个节点运行10个/3台≈3个任务
        修改配置文件
            vim /opt/module/hadoop/hadoop-3.1.3/etc/hadoop/yarn-site.xml 
                追加
                    <!-- 选择调度器，默认容量。 -->
                    <property>
                        <name>yarn.resourcemanager.scheduler.class</name>
                        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
                    </property>
                    <!-- ResourceManager处理调度器请求的线程数量，默认50。如果提交的任务数大于50，可以增加该值，但是不能超过3台*4线程=12线程（去除其它应用程序实际不能超过8）。 -->
                    <property>
                        <name>yarn.resourcemanager.scheduler.client.thread-count</name>
                        <value>8</value>
                    </property>
                    <!-- 是否让YARN自动检测硬件进行配置，默认是false。如果该节点有很多其它应用程序，建议手动配置，否则采用自动。 -->
                    <property>
                        <name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
                        <value>false</value>
                    </property>
                    <!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数。 -->
                    <property>
                        <name>yarn.nodemanager.resource.count-logical-processors-as-cores</name>
                        <value>false</value>
                    </property>
                    <!-- 虚拟核数和物理核数乘数，默认是1.0。 -->
                    <property>
                        <name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
                        <value>1.0</value>
                    </property>
                    <!-- NodeManager使用内存数，默认8G，修改为4G。 -->
                    <property>
                        <name>yarn.nodemanager.resource.memory-mb</name>
                        <value>4096</value>
                    </property>
                    <!-- NodeManager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个。 -->
                    <property>
                        <name>yarn.nodemanager.resource.cpu-vcores</name>
                        <value>4</value>
                    </property>
                    <!-- 容器最小内存，默认1G。 -->
                    <property>
                        <name>yarn.scheduler.minimum-allocation-mb</name>
                        <value>1024</value>
                    </property>
                    <!-- 容器最大内存，默认8G，修改为2G。 -->
                    <property>
                        <name>yarn.scheduler.maximum-allocation-mb</name>
                        <value>2048</value>
                    </property>
                    <!-- 容器小CPU核数，默认1个。 -->
                    <property>
                        <name>yarn.scheduler.minimum-allocation-vcores</name>
                        <value>1</value>
                    </property>
                    <!-- 容器大CPU核数，默认4个，修改为2个。 -->
                    <property>
                        <name>yarn.scheduler.maximum-allocation-vcores</name>
                        <value>2</value>
                    </property>
                    <!-- 虚拟内存检查，默认代开，修改为关闭。 -->
                    <property>
                        <name>yarn.nodemanager.vmem-check-enabled</name>
                        <value>false</value>
                    </property>
                    <!-- 虚拟内存和物理内存设置比例，默认2.1。 -->
                    <property>
                        <name>yarn.nodemanager.vmem-pmem-ratio</name>
                        <value>2.1</value>
                    </property>
        分发配置到各服务器（若其它服务器硬件配置不一样，则需要单独配置）
            略
        重启YARN（ResourceManager节点）
            cd /opt/module/hadoop/hadoop-3.1.3/
            sbin/stop-yarn.sh 
            sbin/start-yarn.sh
        运行程序并观察任务执行页面
            略
    容量调度器
        容量调度器多队列
            概述
                调度器默认就1个default队列，不能满足生产要求。
                如何设计队列
                    按照框架：hive/spark/flink，每个框架的任务放入指定队列。
                    按照业务模块：模块1/模块2/...。
                多队列好处
                    避免坏任务耗尽全部资源
                    实现任务降级，保证重要的任务队列资源充足。
            配置 
                假设default队列占总内存的40%，最大资源容量占总资源60%，hive队列占总内存的60%，最大资源容量占总资源80%。
                修改配置文件
                    vim /opt/module/hadoop/hadoop-3.1.3/etc/hadoop/yarn-site.xml 
                        更新或追加
                            <!-- 选择调度器，默认容量。 -->
                            <property>
                                <name>yarn.resourcemanager.scheduler.class</name>
                                <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
                            </property>
                    vim /opt/module/hadoop/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml
                        更新
                            <!-- 指定多队列，增加hive队列。 -->
                            <property>
                                <name>yarn.scheduler.capacity.root.queues</name>
                                <value>default,hive</value>
                            </property>
                            <!-- 降低default队列资源额定容量为40%，默认100%。 -->
                            <property>
                                <name>yarn.scheduler.capacity.root.default.capacity</name>
                                <value>40</value>
                            </property>
                            <!-- 降低default队列资源最大容量为40%，默认100%。 -->
                            <property>
                                <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
                                <value>60</value>
                            </property>
                        追加
                            <!-- 指定hive队列额定容量 -->
                            <property>
                                <name>yarn.scheduler.capacity.root.hive.capacity</name>
                                <value>60</value>
                            </property>
                            <!-- 指定hive队列最大容量 -->
                            <property>
                                <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>
                                <value>80</value>
                            </property>
                            <!-- 指定hive队列用户最多可以使用队列多少资源。 -->
                            <property>
                                <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>
                                <value>1</value>
                            </property>
                            <!-- 指定hive队列是否启用 -->
                            <property>
                                <name>yarn.scheduler.capacity.root.hive.state</name>
                                <value>RUNNING</value>
                            </property>
                            <!-- 指定hive队列哪些用户可以提交任务 -->
                            <property>
                                <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>
                                <value>*</value>
                            </property>
                            <!-- 指定hive队列哪些用户可以作为管理员 -->
                            <property>
                                <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>
                                <value>*</value>
                            </property>
                            <!-- 指定hive队列哪些用户可以设置任务优先级 -->
                            <property>
                                <name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>
                                <value>*</value>
                            </property>
                            <!-- 指定hive队列最大生命周期 -->
                            <!-- 提交到队列的application的超时时间不能超过application-lifetime，若maximum没有指定则取default。 -->
                            <!-- 例：yarn application -appId <app_id> -updateLifetime <timeout> -->
                            <property>
                                <name>yarn.scheduler.capacity.root.hive.maximum-application-lifetime</name>
                                <value>-1</value>
                            </property>
                            <!-- 指定hive队列默认生命周期 -->
                            <property>
                                <name>yarn.scheduler.capacity.root.hive.default-application-lifetime</name>
                                <value>-1</value>
                            </property>
                分发配置到各服务器 
                    略
                重启YARN（ResourceManager节点）
                    方式一
                        cd /opt/module/hadoop/hadoop-3.1.3/
                        sbin/stop-yarn.sh 
                        sbin/start-yarn.sh
                    方式二（刷新队列）
                        yarn rmadmin -refreshQueues
                提交任务到指定队列
                    hadoop jar /opt/module/hadoop/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /wcinput /wcoutput
        容量调度器任务优先级
            修改配置文件
                vim /opt/module/hadoop/hadoop-3.1.3/etc/hadoop/yarn-site.xml 
                    追加 
                        <!-- 指定优先级为5个等级 -->
                        <property>
                            <name>yarn.cluster.max-application-priority</name>
                            <value>5</value>
                        </property>
            分发配置到各服务器
                略
            重启YARN（ResourceManager节点）
                cd /opt/module/hadoop/hadoop-3.1.3/
                sbin/stop-yarn.sh 
                sbin/start-yarn.sh
            提交任务（假设第一、二、三个任务依次提交，第三个任务设置了优先级，则执行顺序应该是：第一个任务执行，然后第三个任务执行，最后第二个任务执行） 
                hadoop01
                    hadoop jar /opt/module/hadoop/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 5 2000000
                hadoop02
                    hadoop jar /opt/module/hadoop/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 5 2000000
                hadoop03
                    hadoop jar /opt/module/hadoop/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -D mapreduce.job.priority=5 5 2000000
    公平调度器
        公平调度器多队列
            假设有两个队列：queuea和queueb，期望实现以下效果：若用户提交任务时指定队列，则任务提交到指定队列；若未指定队列，usera用户提交的任务到queuea队列，userb提交的任务到queueb队列。
            修改配置文件
                vim /opt/module/hadoop/hadoop-3.1.3/etc/hadoop/yarn-site.xml 
                    追加
                        <!-- 指定使用公平调度器 -->
                        <property>
                            <name>yarn.resourcemanager.scheduler.class</name>
                            <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
                        </property>
                        <!-- 指定公平调度器队列分配文件 -->
                        <property>
                            <name>yarn.scheduler.fair.allocation.file</name>
                            <value>/opt/module/hadoop/hadoop-3.1.3/etc/hadoop/fair-scheduler.xml</value>
                        </property>
                        <!-- 指定禁止队列间抢占资源 -->
                        <property>
                            <name>yarn.scheduler.fair.preemption</name>
                            <value>false</value>
                        </property>
                vim /opt/module/hadoop/hadoop-3.1.3/etc/hadoop/fair-scheduler.xml 
                    新增
                        <?xml version="1.0"?>
                        <allocations>
                            <!-- 单个队列中Application Master占用资源的最大比例，取值0~1，企业一般配置0.1。 -->
                            <queueMaxAMShareDefault>0.5</queueMaxAMShareDefault>
                            <!-- 单个队列最大资源的默认值 -->
                            <queueMaxResourceDefault>4096mb,4vcores</queueMaxResourceDefault>
                            <queue name="test">
                                <minResources>2048mb,2vcores</minResources>
                                <maxResources>4096mb,4vcores</maxResources>
                                <maxRunningApps>4</maxRunningApps>
                                <maxAMShare>0.5</maxAMShare>
                                <weight>1.0</weight>
                                <!-- 队列内资源分配策略 -->
                                <schedulingPolicy>fair</schedulingPolicy>
                            </queue>
                            <queue name="sxydh" type="parent">
                                <minResources>2048mb,2vcores</minResources>
                                <maxResources>4096mb,4vcores</maxResources>
                                <maxRunningApps>4</maxRunningApps>
                                <maxAMShare>0.5</maxAMShare>
                                <weight>1.0</weight>
                                <!-- 队列内资源分配策略 -->
                                <schedulingPolicy>fair</schedulingPolicy>
                            </queue>
                            <!-- 任务队列分配策略，可配置多层规则，从第一个规则开始匹配，直到匹配成功。 -->
                            <queuePlacementPolicy>
                                <!-- 提交任务时如果指定队列则分配到该队列，如果未指定，则继续匹配下一个规则。false：如果指定队列不存在，不允许自动创建。 -->
                                <rule name="specified" create="false" />
                                <!-- 提交任务到root.group.user队列，若root.group不存在，不允许自动创建，若root.group.user不存在，允许自动创建。 -->
                                <rule name="nestedUserQueue" create="true">
                                    <rule name="primaryGroup" create="false" />
                                </rule>
                                <!-- 最后一个规则必须为reject或者default。reject：规则匹配失败则拒绝任务提交。 -->
                                <rule name="reject" />
                            </queuePlacementPolicy>
                        </allocations>
            分发配置到各服务器
                略
            重启YARN（ResourceManager节点）
                cd /opt/module/hadoop/hadoop-3.1.3/
                sbin/stop-yarn.sh 
                sbin/start-yarn.sh
            提交任务 
                指定队列提交
                    hadoop jar /opt/module/hadoop/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -D mapreduce.job.queuename=root.test 1 1
                        提交到队列test
                不指定队列提交
                    hadoop jar /opt/module/hadoop/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 1 1
                        假设当前user为sxydh，则提交到队列root.sxydh。
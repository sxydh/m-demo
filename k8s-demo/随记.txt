概述
    K8s (Kubernetes) 是用于部署、扩展、管理容器的开源系统。
核心概念
    Pod
        Pod 是一组容器的抽象
        Pod 内的不同容器可以通过 localhost 互相访问
    Service
        Service 是一组 Pod 的抽象，是外界对这组 Pod 的访问入口。
            Pod 的新建和销毁可能导致 IP 发生变化，外界直接通过 IP 访问 Pod 就会产生问题。
            Service 服务创建后，外界访问 Service 的 IP ，然后再由 Service 将流量路由到合适的 Pod 。
        Service 对外曝露的方式有：
            ClusterIP 
                以集群内部 IP 对外曝露，这种类型的 Service 只能集群内部访问。
            NodePort
                以每个集群节点端口映射的方式对外曝露，这种类型的 Service 可以从外部访问。
            ...
    Namespace
        Namespace 是一组资源的抽象，提供了一个逻辑隔离的空间。
资源对象
    概述
        资源是 K8s 中所有内容的抽象，如 Pod 、 Service 等都是资源。 K8s 已经内置多种资源，如工作负载、存储资源、网络资源等。
        资源对象就是资源的实例，是持久化的实体，如某个具体的 Pod 、某个具体的 Node 等。 K8s 使用这些实体去表示整个集群的状态。
        资源对象通过配置文件来创建，配置文件是资源对象属性的描述文件，配置文件格式可以是 JSON 、 YAML 等。
    分类
        基本资源
            Pod
            Service
        工作负载
            Deployment
            StatefulSet
            DaemonSet
            ReplicaSet
        配置存储
            ConfigMap
                ConfigMap 用于存储键值对，创建之后可以被 Pod 以多种方式引用，例如作为 Pod 的环境变量，或者挂载到 Pod 内部的配置文件。
                ConfigMap 更新后，以环境变量引用的方式需要重启 Pod 才会生效，以文件挂载的方式不需要重启。
            Secret
            emptyDir
                emptyDir 可作为 Pod 的存储
                emptyDir 的生命周期与 Pod 一致
            PersistentVolume
            PersistentVolumeClaim
        网络资源
            Ingress
            NetworkPolicy
        集群管理
            Node
            Namespace
        用户权限
            Role
        ...
    定制
        资源可以自定义（ Custom Resource Definition ， CRD ）
        CRD 创建后，在 K8s 内仅表现为一个被存储的定义，需要借助用户实现的 Controller 来进一步发挥其作用。
        Controller 可以监听 CRD 资源对象的增删改查事件，用户可以根据这些事件来做不同的处理。
核心组件
    Kubernetes Master
        API Server
            API Server 向外部客户端暴露 K8s 的 API ，外部客户端可以通过这些 API 来发出请求以处理自己的工作。
        Scheduler
            Scheduler 根据集群 Node 的资源信息，将 Pod 调度到合适的 Node 上运行。
        Controller
            Replication Controller
                Replication Controller 用于确保 Pod 的副本数量与期望数量一致，并监控 Pod 的健康状态。
            Node Controller
                Node Controller 用于确保集群中 Node 数量与期望数量一致，并监控 Node 的健康状态。
            ...
        ETCD
            ETCD 是 K8s 集群的后台数据库，内部实现是键值存储，类似 Redis 。
            ETCD 存储的数据包括集群配置、节点状态信息、服务信息、 Pod 信息、事件信息等。
    Kubernetes Node
        Kubelet
            Kubelet 负责与 API Server 通信、容器生命周期管理（通过 Container Runtime 实现）、资源信息（例如计算、内存、存储、网络资源）上报等。
        Kube-proxy
            Kube-proxy 可以实现流量负载均衡的功能，将到达 Service 的流量分配到合适的 Pod 。
                对 Service 的访问可以使用 NodePort 方式
                    NodePort 是将每个节点指定端口的流量，转发到指定的 Service 。
            Kube-proxy 提供的负载均衡主要有两种场景：
                节点外客户端访问 Service
                节点内 Pod 访问 Service
        Container Runtime
            Container Runtime （容器运行时）主要用于容器生命周期的管理，包含容器创建、启动、运行、停止、销毁等，容器运行时的具体实现有 Containerd 、 CRI-O 等。
功能组件
    网络
        Ingress
            Ingress 可以实现流量路由、负载均衡的功能，将到达 Ingress 的流量转发到合适的 Service 。
            Ingress 的功能类似于 Nginx ，先收到外部客户端的请求（一般带有域名），然后根据用户定义的规则，转发到正确的 Service 。
            Ingress 组件包含 Ingress 、 Reverse Proxy 、 Ingress Controller 。
                Ingress 是路由规则对象
                Reverse Proxy 负责流量路由，支持多种实现，例如 Nginx 等。
                Ingress Controller 负责感知路由规则的变化，并动态应用到 Reverse Proxy 。
            
        Flannel
        Calico

        相关知识
            OSI
                7 Application layer
                6 Presentation layer
                5 Session layer
                4 Transport layer
                    UDP
                    TCP
                3 Network layer
                    IP
                2 Data link layer
                    Ethernet
                    VLAN
                1 Physical layer
            NAT
            Underlay
                物理网络
            Overlay
                虚拟网络
            LAN
                LAN （ Local Area Network ，局域网）是指覆盖范围较小的网络
            VLAN
                VLAN （ Virtual Local Area Network ，虚拟局域网）是将一个 LAN 局域网进行划分，形成多个逻辑上独立的虚拟局域网。
            交换机
                二层交换机
                    二层交换机主要用于二层网络（链路层）的互通
                三层交换机
                    三层交换机可以认为是带有路由功能的二层交换机，即用于三层网络（网络层）的互通。
    存储
        分类
            对象存储 OSS
                协议支持 HTTP/HTTPS
            文件存储 NAS
                协议支持 NFS/SMB
            块存储 EBS
                协议支持 SCSI
        实现
            Ceph
    监控
        Prometheus
        Grafana
        ELK
            Elasticsearch
            Logstash
            Kibana
环境
    版本 1.23.9 （ https://bbs.huaweicloud.com/blogs/380596 ）
        前置条件
            Ubuntu 20.04.6 LTS
                192.168.233.129   hadoop01
                192.168.233.130   hadoop02
                192.168.233.131   hadoop03
        
                sudo vim /etc/hostname
                    配置主机名
                    三台主机都要进行配置
                    需要重启
                sudo vim /etc/hosts
                    配置主机列表
                    三台主机都要进行配置
                    追加或更新
                        192.168.233.129 hadoop01
                        192.168.233.130 hadoop02
                        192.168.233.131 hadoop03
                    需要重启
            规划资源
                Master   hadoop01   192.168.233.129
                Node     hadoop02   192.168.233.130
                Node     hadoop03   192.168.233.131
            用户权限
                sudo su
            关闭 Swap （ hadoop01/hadoop02/hadoop03 ， Swap 会影响容器性能）
                swapoff -a
                sed -ri 's/.*swap.*/#&/' /etc/fstab
                    -r ：启用正则表达式。
                    -i ：原地编辑。
                    's/.*swap.*/#&/' ：替换命令，在包含 swap 的行前加 # ，即注释掉该行。
                    /etc/fstab ：要编辑的文件路径。
            桥接网络（ hadoop01/hadoop02/hadoop03 ，确保容器网络进出都经过 ip6tables/iptables ，以应用网络安全规则）
                cat > /etc/sysctl.d/k8s.conf << EOF
                net.bridge.bridge-nf-call-ip6tables = 1
                net.bridge.bridge-nf-call-iptables = 1
                EOF
        
                sysctl --system
        
        安装 Docker （ hadoop01/hadoop02/hadoop03 ， 20.10.16 ）
            略
                参考 docker-demo
        
            systemctl enable docker
                添加开机自启
        
            vim /etc/docker/daemon.json
                追加
                    {
                      ...
                      "exec-opts": ["native.cgroupdriver=systemd"]
                    }
                        告诉 Docker 使用 systemd 作为其 cgroup 驱动程序，这样做的目的是为了与宿主系统的 cgroup 配置保持一致，以便更好地管理容器的资源。
        
                systemctl daemon-reload
                    重新加载 systemd 守护程序的配置文件
                systemctl restart docker
        
        安装 K8s 核心组件（ hadoop01/hadoop02/hadoop03 ）
            curl http://mirrors.huaweicloud.com/kubernetes/yum/doc/apt-key.gpg | apt-key add -
                添加 K8s GPG 密钥到受信任列表
        
            cat > /etc/apt/sources.list.d/kubernetes.list << EOF
            deb http://mirrors.huaweicloud.com/kubernetes/apt/ kubernetes-xenial main
            EOF
                添加 k8s 镜像源
        
            apt update
            apt install -y kubelet=1.23.9-00 kubeadm=1.23.9-00 kubectl=1.23.9-00
            systemctl enable kubelet
        
        启动 K8s 集群（ hadoop01 ）
            kubeadm init \
            --apiserver-advertise-address=192.168.233.129 \
            --image-repository registry.aliyuncs.com/google_containers \
            --pod-network-cidr=10.244.0.0/16 \
            --ignore-preflight-errors='all'
                初始化 K8s Master ，执行之后需要按照控制台提示进行操作。
    
            kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
                安装网络组件 Flannel ，为集群提供虚拟网络，让各节点上的 Pod 之间可以互相通信。
                建议 docker 使用 socks5 代理的情况下安装 Flannel
                如果安装失败，通过以下命令删除后再重试：
                    kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
        
        加入 K8s 集群（ hadoop02/hadoop03 ）
            kubeadm join 192.168.233.129:6443 --token <token> \
                    --discovery-token-ca-cert-hash sha256:<sha256>
        
        安装 K8s Dashboard （ hadoop01 ， 2.5.1 ， https://skyao.io/learning-kubernetes/docs/installation/kubeadm/dashboard ）
            kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml
                安装完成后，是以 ClusterIP 的形式对外曝露服务，只能在集群内部访问，需要将 ClusterIP 更新为 NodePort ：
                    kubectl edit service kubernetes-dashboard -n kubernetes-dashboard
                        更新
                            spec:
                                ...
                                type: NodePort
                查看 Dashboard 所在节点：
                    kubectl get pods -A -o wide | grep kubernetes-dashboard
                查看 Dashboard 所在端口：
                    kubectl -n kubernetes-dashboard get service kubernetes-dashboard
                获取 Dashboard 登录 token
                    kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token
                登录 Dashboard
                    https://<ip>:<port>
                        如果浏览器因为安全问题无法进入，可以在页面空白处输入以下内容即可：
                            thisisunsafe
        
        部署应用
            简单示例
                kubectl create deployment nginx --image=nginx
                    创建部署
        
                    kubectl expose deployment nginx --port=80 --type=NodePort
                        服务曝露
                    kubectl scale -n default deployment nginx --replicas=2
                        服务伸缩
                    验证
                        kubectl get service -A -o wide
                            服务端口查看
                                http://<any_cluster_node_ip>:<port>
                                
    版本 1.30 （ https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ ）
        前置条件
            Ubuntu 20.04.6 LTS
                192.168.233.129 hadoop01
                192.168.233.130 hadoop02
                192.168.233.131 hadoop03
            规划资源
                Kubernetes Master
                    hadoop01
                Kubernetes Node
                    hadoop02
                    hadoop03
            用户权限
                sudo su
            配置容器运行时（ hadoop01, hadoop02, hadoop03 ）
                Containerd 1.7.20
                    配置 CGroup
                        概述
                            CGroup 用于限制分配给进程的资源
                            容器运行时的 CGroup 和 Kubelet 的 CGroup 需要保持一致
                        开始
                            mkdir -p /etc/containerd
                            containerd config default | tee /etc/containerd/config.toml
                            vim /etc/containerd/config.toml
                                更新
                                    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
                                        SystemdCgroup = true
                            systemctl daemon-reload
                            systemctl restart containerd
                            
                    配置 Registry
                        建议初始化集群之后再配置
                                
            关闭 Swap （ hadoop01, hadoop02, hadoop03 ）
                概述
                    Swap 开启时， Kubelet 无法启动。
                开始
                    swapoff -a
                    vim /etc/fstab
                        更新
                            #/swap.img      none    swap    sw      0       0
                            
            配置网络（ hadoop01, hadoop02, hadoop03 ）
                cat > /etc/sysctl.d/k8s.conf << EOF
                net.bridge.bridge-nf-call-ip6tables = 1
                net.bridge.bridge-nf-call-iptables  = 1
                net.ipv4.ip_forward                 = 1
                EOF
                
                    net.bridge.bridge-nf-call-ip6tables ：当数据包通过桥接设备时，是否检查 ip6tables 规则。
                    net.bridge.bridge-nf-call-iptables ：当数据包通过桥接设备时，是否检查 iptables 规则。
                    net.ipv4.ip_forward ：是否允许将收到的数据包转发到其它网络接口。
        
                sysctl --system
                                
            配置代理
                export http_proxy=http://192.168.62.185:10809
                export https_proxy=http://192.168.62.185:10809
                        
        安装 Kubeadm 工具箱（ hadoop01, hadoop02, hadoop03 ）
            概述
                Kubeadm 工具箱用于创建 Kubernetes 集群
            开始
                apt-get update
                apt-get install -y apt-transport-https ca-certificates curl gpg
                    安装 K8s 依赖包
                    
                mkdir -p -m 755 /etc/apt/keyrings
                curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
                    下载 K8s 公钥
                    
                echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list
                    添加 K8s 仓库
                    此操作会覆盖 /etc/apt/sources.list.d/kubernetes.list 中现存的所有配置
                    
                apt-get update
                apt-get install -y \
                    -o Acquire::http::Proxy="http://192.168.62.185:10809" \
                    -o Acquire::https::Proxy="http://192.168.62.185:10809" \
                    kubelet kubeadm kubectl
                apt-mark hold kubelet kubeadm kubectl
                    锁定版本
        创建 K8s 集群
            概述
                使用 Kubeadm 工具箱创建 Kubernetes 集群
            开始
                初始化 Kubernetes Master （ hadoop01 ）
                
                    ctr -n k8s.io image pull registry.k8s.io/kube-apiserver:v1.30.3
                    ctr -n k8s.io image pull registry.k8s.io/kube-controller-manager:v1.30.3
                    ctr -n k8s.io image pull registry.k8s.io/kube-scheduler:v1.30.3
                    ctr -n k8s.io image pull registry.k8s.io/kube-proxy:v1.30.3
                    ctr -n k8s.io image pull registry.k8s.io/coredns/coredns:v1.11.1
                    ctr -n k8s.io image pull registry.k8s.io/pause:3.9
                    ctr -n k8s.io image pull registry.k8s.io/pause:3.8
                    ctr -n k8s.io image pull registry.k8s.io/etcd:3.5.12-0
                    
                        先拉取初始化 Master 所需要的镜像列表，避免后续步骤由于拉取问题引起失败，该镜像列表可通过以下命令获取：
                            kubeadm config images list
                            
                    kubeadm init \
                        --apiserver-advertise-address=192.168.233.129 \
                        --pod-network-cidr=10.244.0.0/16 \
                        --ignore-preflight-errors='all'
                    
                            --apiserver-advertise-address ：设置控制平面节点 API 服务器的广播地址。
                            --pod-network-cidr ：设置 Pod 网络地址范围。
                            
                            初始化 Master ，建议退出代理。
                            初始化完成后需要按照控制台提示执行相关操作：
                                mkdir -p $HOME/.kube
                                cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
                                chown $(id -u):$(id -g) $HOME/.kube/config
                                export KUBECONFIG=/etc/kubernetes/admin.conf
                                
                    journalctl -u kubelet -n 100 -f
                    journalctl -u containerd -n 100 -f
                            
                部署 Pod 网络插件（ hadoop01 ）
                    概述
                        必须部署一个基于 Pod 网络插件的 CNI （ Container Network Interface ）， 以便于 Pod 间可以相互通信。
                        一个集群只能部署一个 Pod 网络插件
                    开始
                        
                        ctr -n k8s.io image pull docker.io/flannel/flannel-cni-plugin:v1.5.1-flannel1
                        ctr -n k8s.io image pull docker.io/flannel/flannel:v0.25.5
                            
                            先拉取部署 CNI 所需要的镜像，避免后续步骤由于拉取问题引起失败。
                        
                        kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
                        kubectl get pod -o wide -n kube-flannel
                        
                初始化 Kubernetes Node （ hadoop02, hadoop03 ）
                
                    ctr -n k8s.io image pull registry.k8s.io/coredns/coredns:v1.11.1
                    ctr -n k8s.io image pull registry.k8s.io/pause:3.9
                    ctr -n k8s.io image pull registry.k8s.io/pause:3.8
                    ctr -n k8s.io image pull registry.k8s.io/kube-proxy:v1.30.3
                    ctr -n k8s.io image pull docker.io/flannel/flannel-cni-plugin:v1.5.1-flannel1
                    ctr -n k8s.io image pull docker.io/flannel/flannel:v0.25.5
                    
                        先拉取初始化 Node 所需要的镜像，避免后续步骤由于拉取问题引起失败。
                
                    kubeadm join 192.168.233.129:6443 \
                        --token <token> \
                        --discovery-token-ca-cert-hash sha256:<sha256>
                        
                            token =
                                kubeadm token list
                                    如果过期可以重新创建：
                                        kubeadm token create
                            sha256 =
                                openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | \
                                    openssl dgst -sha256 -hex | sed 's/^.* //'
                                    
                            初始化 Node ，建议退出代理。
                                    
                    journalctl -u kubelet -n 100 -f
                    
                开启 ipvs
                    概述
                        ipvs （ IP Virtual Server ）用于实现传输层负载均衡
                        K8s 默认使用 iptables 而不是 ipvs
                        开启 ipvs 原因：
                            不开启 ipvs 会出现以下问题，同一个 Node 下 Pod 与 Pod 可以通信，但是 Pod 与 Service 无法通信，目前无法解决。
                    开始（ https://juejin.cn/post/7158567812011655204 ）
                        安装依赖（ hadoop01, hadoop02, hadoop03 ）
                            apt-get update
                            apt-get install -y ipvsadm ipset
                        更新应用（ hadoop01 ）
                            kubectl edit configmap kube-proxy -n kube-system
                                更新
                                    mode: "ipvs"
                                    iptables:
                                      # 控制源地址转换
                                      masqueradeAll: true
                            kubectl delete pod --all -n kube-system
                            kubectl get pod -o wide -n kube-system
                            
                            POD_NAME=$(kubectl get pod -n kube-system | grep kube-proxy | awk 'NR==1 {print $1}') && kubectl logs $POD_NAME -n kube-system
                            POD_NAME=$(kubectl get pod -n kube-system | grep kube-proxy | awk 'NR==2 {print $1}') && kubectl logs $POD_NAME -n kube-system
                            POD_NAME=$(kubectl get pod -n kube-system | grep kube-proxy | awk 'NR==3 {print $1}') && kubectl logs $POD_NAME -n kube-system
                            
                            ipvsadm -ln
                    
        卸载 K8s 集群
            https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#tear-down
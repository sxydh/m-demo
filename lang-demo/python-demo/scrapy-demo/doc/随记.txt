概述
    Scrapy 是网络爬虫框架
    https://scrapy.org/
架构
    核心组件
    https://www.runoob.com/w3cnote/scrapy-detail.html
        Spider
            概述
                负责处理响应数据包 Response ，并将处理结果 Item 通过 Scrapy Engine 传递给 Item Pipeline 。
                Spider 除了可以返回 Item ，还可以产生新的网络请求 Request ，并通过 Scrapy Engine 传递给 Scheduler 。
        Scheduler
            概述
                负责接收网络请求 Request ，放入调度队列，再从队列中取出 Request ，通过 Scrapy Engine 传递给 Downloader 。
        Downloader
            概述
                负责处理网络请求 Request ，并将响应数据包 Response 通过 Scrapy Engine 传递给 Spider 。
        Item Pipeline
            概述
                负责处理结果数据 Item ，例如数据分析、持久化等。
        Scrapy Engine
            概述
                负责 Spider/Scheduler/Downloader/Item Pipeline 之间的数据通信
        Middleware
            概述
                负责 Spider/Downloader 的预处理，类似拦截器。
    基本过程
        1 - Spider 产生新的网络请求 Request ，交给 Scheduler 调度。
        2 - Scheduler 调度 Request 给 Downloader
        3 - Downloader 获取响应数据包 Response ，交给 Spider 处理。
        4 - Spider 处理 Response 得到结果数据 Item ，交给 Item Pipeline 处理。
        5 - Item Pipeline 处理 Item
        注意：以上过程可以形成一个循环。
开始
    单机
        创建项目
            开始
                pip install -i https://pypi.tuna.tsinghua.edu.cn/simple Scrapy
                scrapy startproject tjbz
                cd tjbz
                scrapy genspider example stats.gov.cn
            项目结构
                tjbz
                    scrapy.cfg
                        定义项目部署配置
                    tjbz
                        items.py
                            概述
                                定义爬取数据的数据结构
                        middlewares.py
                            概述
                                定义中间件，包含 Spider Middleware/Downloader Middleware 等。
                            开始
                                Spider Middleware
                                    概述
                                        可以用于设置 Request 的 User-Agent/Proxy 等。
                                Downloader Middleware
                                    概述
                                        可以用于实现响应内容的动态渲染等
                        pipelines.py
                            概述    
                                定义数据处理管道
                        settings.py
                            概述
                                定义项目运行配置
                        spiders
                            example.py
                                概述
                                    定义用户 Spider
        定义设置
            USER_AGENT = "tjbz (+http://www.yourdomain.com)"
                定义 Request 的请求头 User-Agent
            ROBOTSTXT_OBEY = False
                定义是否遵守 Robot 爬虫约定
                
            SPIDER_MIDDLEWARES = {
                "tjbz.middlewares.TjbzSpiderMiddleware": 543,
            }
            
                定义 Spider Middleware 链，其中 543 代表顺序优先级。
                
            DOWNLOADER_MIDDLEWARES = {
                "tjbz.middlewares.TjbzDownloaderMiddleware": 543,
            }
            
                定义 Downloader Middleware 链，其中 543 代表顺序优先级。
            
            ITEM_PIPELINES = {
                "tjbz.pipelines.TjbzPipeline": 300,
            }
            
                定义 Item Pipeline 链，其中 300 代表顺序优先级。
                
            HTTPERROR_ALLOWED_CODES = [301]
                定义可以被用户 Spider 处理的额外状态码
        定义实体
            items.py
                class TjbzItem(scrapy.Item):
                    ...
        构造请求
            tjbz.py
                class TjbzSpider(scrapy.Spider):
                    name = "tjbz"
                    allowed_domains = ["stats.gov.cn"]
                    start_urls = ["https://www.stats.gov.cn/sj/tjbz/tjyqhdmhcxhfdm/2023/index.html"]
                
                    def start_requests(self) -> Iterable[Request]:
                        # 构造初始请求，父类已经实现，一般情况下用户不需要覆盖。
                        # 用户可以覆盖该函数，在这里设置 Cookie 等信息。
                        ...
                
                    def parse(self, response: Response, **kwargs: Any):
                        ...
                        yield scrapy.Request(url=..., callback=..., meta=...)
                        
                            url ：目标地址。
                            callback ：回调函数。
                            meta ：用户参数，用于在不通的解析函数中通信。 
                                    
        解析响应
            tjbz.py
                class TjbzSpider(scrapy.Spider):
                    ...
                
                    def parse(self, response: Response, **kwargs: Any):
                        ...
                        yield {
                            ...
                        }
                
        保存实体
            pipelines.py
                class TjbzPipeline:
                
                    def open_spider(self, spider):
                        # 生命周期函数，仅执行一次。
                        ...
                
                    def process_item(self, item, spider):
                        return item
                        
                            注意： Pipeline 是链式处理，需要显示返回值。
                            
                    def close_spider(self, spider):
                        # 生命周期函数，仅执行一次。
                        ...
                            
        定义中间件（可选）
            middlewares.py
                class TjbzSpiderMiddleware:
                    def process_spider_output(self, response, result, spider):
                        # Spider 输出预处理
                        # 需要显示返回 iterable of Request/item objects
                        ...
                    def process_spider_input(self, response, spider):
                        # Spider 输入预处理
                        ...
                class TjbzDownloaderMiddleware:
                    def process_request(self, request, spider):
                        # Downloader 请求预处理
                        # 可以用于设置 Cookie/User-Agent/Proxy 等，或者将 Request 给第三方组件（ 例如： Selenium ）执行。
                        # 如果返回 None ，则继续执行该 Request 。
                        # 如果返回 Response ，则交给 Spider 处理。
                        # 如果返回 Request ，则交给 Scheduler 处理。
                        ...
                    def process_response(self, request, response, spider):
                        # Downloader 响应预处理
                        # 可以用于处理第三方组件得到的响应
                        # 如果返回 Response ，则交给 Spider 处理。
                        # 如果返回 Request ，则交给 Scheduler 处理。
                        ...
                    def process_exception(self, request, exception, spider):
                        # Downloader 异常预处理
                        ...
        启动爬虫
            scrapy crawl tjbz -L INFO
    断点续爬
        概述
            https://github.com/rmax/scrapy-redis
            基于 Redis 实现请求调度，避免机器重启后，重复调度已经处理的请求。
        开始
            定义设置
                settings.py
                    DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
                        定义重复过滤器，基于 Redis 实现。
                    SCHEDULER = "scrapy_redis.scheduler.Scheduler"
                        定义请求调度器，基于 Redis 实现。
                    SCHEDULER_PERSIST = True
                        定义爬虫结束后是否保留去重集合和调度队列
                    REDIS_URL = "redis://192.168.233.129:6379"
                        定义 Redis 服务器
            定义实体
                ...
            构造请求
                demoz.py
                    class DmozSpider(CrawlSpider):
                        # CrawlSpider 继承自 Spider ，增加了按规则提取链接等功能。
                        ...
            解析响应
                ...
            保存实体
                ...
            启动爬虫
                scrapy crawl demoz
    分布式
        概述
            https://github.com/rmax/scrapy-redis
            基于 Redis 实现请求调度，避免多个机器运行时，重复调度已经处理的请求。
        开始
            创建项目
                ...
            定义设置
                settings.py
                    SPIDER_MODULES = ["example.spiders"]
                    NEWSPIDER_MODULE = "example.spiders"
                
                    DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
                        定义重复过滤器，基于 Redis 实现。
                    SCHEDULER = "scrapy_redis.scheduler.Scheduler"
                        定义请求调度器，基于 Redis 实现。
                    SCHEDULER_PERSIST = True
                        定义爬虫结束后是否保留去重集合和调度队列
                    ITEM_PIPELINES = {
                        "example.pipelines.ExamplePipeline": 300,
                        "scrapy_redis.pipelines.RedisPipeline": 400,
                    }
                    
                        定义实体处理管道链
                    
                    REDIS_URL = "redis://192.168.233.129:6379"
                        定义 Redis 服务器
                        
                    DOWNLOAD_DELAY = 1
                        定义请求间隔
            定义实体
                ...
            构造请求
                myspider_redis.py
                    class MySpider(RedisSpider):
                    
                        redis_key = "myspider:start_urls"
                    
                        def __init__(self, *args, **kwargs):
                            # Dynamically define the allowed domains list.
                            domain = kwargs.pop("domain", "")
                            self.allowed_domains = list(filter(None, domain.split(",")))
                                # 注意： Python3 需要显示 list(...) 以转成列表。
                            super().__init__(*args, **kwargs)
            解析响应
                ...
            保存实体
                ...
            启动爬虫
                scrapy crawl myspider_redis
                
                    1 - 在每个节点上执行上述启动命令
                    2 - 在 Redis 执行 lpush <redis_key> <start_urls>
                
    Splash
        概述
            Splash 用于渲染网页的脚本内容，是 Scrapy 的扩展组件。
            https://www.bilibili.com/video/BV1WY4y1s7Ns?p=33&vd_source=c866e830ebc8bae23e4374812c76e44d
        开始
            安装服务
                docker pull scrapinghub/splash
                ...
            创建项目
                ...
            定义设置
                settings.py
                    SPLASH_URL = 'http://192.168.233.129:8050'
                        定义 Splash 服务
                    DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
                        定义重复过滤器
                    HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'
                        定义 HTTP 缓存
                        
                    DOWNLOADER_MIDDLEWARES = {
                        'scrapy_splash.SplashCookiesMiddleware': 723,
                        'scrapy_splash.SplashMiddleware': 725,
                        'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
                    }
                    
                        定义 Downloader 中间件
                    
                    ROBOTSTXT_OBEY = False
                        定义是否遵守 Robot 爬虫约定
            定义实体
                ...
            构造请求
                ...
                    ...(scrapy.Spider):
                    
                        def start_requests(self) -> Iterable[Request]:
                            ...
                            yield SplashRequest(url=self.start_urls[0],
                                                callback=...,
                                                args={'wait': 10}, # 最大超时时间 10 秒
                                                endpoint='render.html')
            解析响应
                ...
            保存实体
                ...
    Scrapyd
        概述
            Scrapyd 是部署运行 Scrapy 爬虫的框架
            https://www.bilibili.com/video/BV1WY4y1s7Ns?p=38&vd_source=c866e830ebc8bae23e4374812c76e44d
        开始
            ...
    Gerapy
        概述
            Gerapy 是爬虫管理框架
            Gerapy 基于 Scrapy/Scrapyd/Scrapyd-Client/Scrapy-Redis/Scrapyd-API/Scrapy-Splash/Jinjia2/Django/Vue.js 开发
            https://www.bilibili.com/video/BV1WY4y1s7Ns?p=39&vd_source=c866e830ebc8bae23e4374812c76e44d
        开始
            ...
    Appium
        概述
            Appium 是 App 爬虫框架
            https://www.bilibili.com/video/BV1WY4y1s7Ns?p=40&vd_source=c866e830ebc8bae23e4374812c76e44d
        开始
            ...